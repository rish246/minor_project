{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "from xml.etree import ElementTree\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'dataset_name': 'PAN 2018 English',\n",
    "    'xmls_directory': '../data/pan18-author-profiling-training-dataset-2018-02-27/en/text/',\n",
    "    'truth_path': '../data/pan18-author-profiling-training-dataset-2018-02-27/en/en.txt',\n",
    "    'txts_destination_directory': '../data/pan18-author-profiling-training-dataset-2018-02-27/en',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pan_data(xmls_directory, truth_path, txts_destination_directory):\n",
    "    \"\"\" \n",
    "    Loads Pan data\n",
    "\n",
    "    @return: \n",
    "    1. merged tweets of authors\n",
    "    2. truths(read genders of the authors)\n",
    "    3. author ids\n",
    "    4. original tweet lengths of authors\n",
    "\n",
    "    @TODO:\n",
    "        Make sure that the data is read properly and is not misaligned\n",
    "    \"\"\"\n",
    "\n",
    "    # read tweets of the authors\n",
    "    # read the filenames from the xmls_dir\n",
    "    xmls_filenames = sorted(os.listdir(xmls_directory))\n",
    "\n",
    "    # xml filename = (author_id.xml)\n",
    "    # to ge author id: split(filename)[0]\n",
    "    author_ids = []\n",
    "    for xml_filename in xmls_filenames:\n",
    "        author_ids.append(xml_filename[:-4])\n",
    "\n",
    "    #### truths --> go to truth location\n",
    "    # split(:::)\n",
    "    # get the first element\n",
    "    # make sure that order is same in both cases, we are mapping the same things\n",
    "    # since, author ids are sorted, hence truth file should be sorted as well\n",
    "    truths_temp = []\n",
    "    with open(truth_path, 'r') as truth_file:\n",
    "        # sort ids\n",
    "        for line in sorted(truth_file):\n",
    "\n",
    "            line.rstrip('\\n')\n",
    "\n",
    "            entry = line.split(':::')\n",
    "            truths_temp.append(entry)\n",
    "\n",
    "    truths = []\n",
    "    # make sure allignment is correct\n",
    "    for author_idx, truth_vector in enumerate(truths_temp):\n",
    "\n",
    "        if(author_ids[author_idx] != truth_vector[0]):\n",
    "            print(author_ids[author_idx], truths_temp[0])\n",
    "            print(\"Ids in truths file and Ids in author ids array do not align\")\n",
    "            return\n",
    "        truths.append(truth_vector[1])\n",
    "\n",
    "    ############# truths are constructed as well ###########################\n",
    "    ##### form: merged tweets and original tweet lengths of authors ########\n",
    "\n",
    "    # files are xml files \n",
    "    # we need ElementTree module\n",
    "    original_tweet_lengths = []\n",
    "\n",
    "    merged_tweets = []\n",
    "\n",
    "    # get filenames and read tweets\n",
    "    for author_idx, xml_filename in enumerate(xmls_filenames):\n",
    "        # read filename\n",
    "        # construct tree of xml\n",
    "        tree = ElementTree.parse(os.path.join(xmls_directory, xml_filename), parser = ElementTree.XMLParser(encoding = 'utf-8'))\n",
    "\n",
    "        # get the root element of file\n",
    "        root = tree.getroot()\n",
    "\n",
    "        original_tweet_lengths.append([])\n",
    "\n",
    "        tweets_of_this_author = []\n",
    "\n",
    "        # root[0] --> first level of the tree\n",
    "        # since the tweets are in 1st level \n",
    "        for child in root[0]:\n",
    "\n",
    "            tweet = child.text \n",
    "\n",
    "            original_tweet_lengths[author_idx].append(len(tweet))\n",
    "\n",
    "            # replace \\n with lineFeed\n",
    "            tweet.replace('\\n', '<LineFeed>')\n",
    "\n",
    "            tweets_of_this_author.append(tweet)\n",
    "\n",
    "        \n",
    "        # store tweets as string\n",
    "        # string separated by <EndOfTweet> \n",
    "        merged_tweets_of_this_author = \"<EndOfTweet>\".join(tweets_of_this_author)+\"<EndOfTweet>\"\n",
    "\n",
    "        merged_tweets.append(merged_tweets_of_this_author)\n",
    "\n",
    "    return merged_tweets, truths, author_ids, original_tweet_lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the project...\n",
      "Loaded Pan data\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Starting the project...\")\n",
    "\n",
    "    ### 1 -> Read the data from the files\n",
    "merged_tweets, truths, author_ids, original_tweet_lengths = load_pan_data(config['xmls_directory'], config['truth_path'], config['txts_destination_directory'])\n",
    "print(\"Loaded Pan data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed train test split\n"
     ]
    }
   ],
   "source": [
    "docs_train, docs_test, y_train, y_test, author_ids_train, author_ids_test, original_tweet_lengths_train, original_tweet_lengths_test\\\n",
    "    = train_test_split(merged_tweets, truths, author_ids, original_tweet_lengths, test_size = 0.4, random_state = 42, stratify = truths)\n",
    "print(\"Performed train test split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/rishabh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/rishabh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "################## Better tweet preprocesser ######################\n",
    "\n",
    "# import main libraries\n",
    "import nltk\n",
    "from nltk import word_tokenize, FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import TweetTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yo life is good xoxo funlyf amazing ramsharam'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tweet = \"Yo.. Life is 2 good xoxo #fun3Lyf #amazing @Ramsharam\"\n",
    "\n",
    "\n",
    "# preprocess this\n",
    "#1. remove digits and lowercase the words\n",
    "my_tweet = re.sub('\\d', '', my_tweet)\n",
    "#2. lowercase digits\n",
    "my_tweet = my_tweet.lower()\n",
    "\n",
    "\n",
    "\n",
    "######## remove punctuations ###############\n",
    "def remove_punctuation(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', (word))\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "my_tweet= ' '.join(remove_punctuation(my_tweet.split(' ')))\n",
    "\n",
    "###############################################\n",
    "# Lemmatization and tokenization using TweetTokenizer\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "my_tweet_words = []\n",
    "for word in tokenizer.tokenize(my_tweet):\n",
    "    my_tweet_words.append(lemmatizer.lemmatize(word))\n",
    "\n",
    "my_tweet = ' '.join(my_tweet_words)\n",
    "\n",
    "\n",
    "my_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(my_tweet):\n",
    "    \"\"\"\n",
    "    This function will preprocess the input tweet\n",
    "\n",
    "    Steps for preprocessing:\n",
    "        1. Lowercase the letters\n",
    "        2. Replace the characters with frequency greater than 3 with 3 in a word\n",
    "        3. Replace a url with Tag: <URLURL>\n",
    "        4. Replace a tag mention: <UsernameMention>\n",
    "\n",
    "    \n",
    "    @TODO:\n",
    "        1. Look for better preprocessing methods on the web\n",
    "        2. Apply here\n",
    "    \"\"\"\n",
    "    # steps 1 and 2 done\n",
    "    # 1. Remove digits and lowercase the text\n",
    "    # remove digits with empty string\n",
    "    my_tweet = re.sub('\\d', '', my_tweet)\n",
    "    #2. lowercase digits\n",
    "    my_tweet = my_tweet.lower()\n",
    "\n",
    "    tokenizer = TweetTokenizer(preserve_case = False, reduce_len = True)\n",
    "    tokens = tokenizer.tokenize(my_tweet)\n",
    "    \n",
    "    for index, token in enumerate(tokens):\n",
    "        if(token[0:8] == \"https://\"):\n",
    "            \n",
    "\n",
    "\n",
    "    ######## remove punctuations ###############\n",
    "    ### remove @username mentions with <UsernameMention>\n",
    "    for \n",
    "\n",
    "\n",
    "    return preprocessed_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(docs_train, docs_test, perform_dimensionality_reduction):\n",
    "    \"\"\" \n",
    "    We will extract features from the dataset, preprocess it and return the X_train and X_test\n",
    "    \n",
    "    @return:\n",
    "        1. X_train: Feature matrix for training data\n",
    "        2. X_test: Feature matrix for test data\n",
    "\n",
    "\n",
    "    @Regions of improvement:\n",
    "        1. Get more features and use them to get more accurate predictions \n",
    "    @TODO:\n",
    "        We are only taking word-ngrams, Use char-ngrams in the features as well\n",
    "    \"\"\"\n",
    "    word_ngram_range = (1, 3)\n",
    "    # perform_dimensionality_reduction = True\n",
    "    # print(docs_test[0])\n",
    "\n",
    "    '''\n",
    "    Build a char_vectorizer and combine word_vectorizer and char_vectorizer to make an n_grams vectorizer\n",
    "    '''\n",
    "\n",
    "    word_vectorizer = TfidfVectorizer(preprocessor=preprocess_tweet,\n",
    "                                    analyzer='word',\n",
    "                                    ngram_range=word_ngram_range,\n",
    "                                    min_df=2,\n",
    "                                    use_idf=True, \n",
    "                                    sublinear_tf=True)\n",
    "    print(\"Created a word vectorizer\")\n",
    "\n",
    "    char_vectorizer = TfidfVectorizer(preprocessor=preprocess_tweet,\n",
    "                                     analyzer='char', \n",
    "                                     ngram_range=(3, 5),\n",
    "                                     min_df=2, \n",
    "                                     use_idf=True, \n",
    "                                     sublinear_tf=True)\n",
    "    print(\"Created a character vectorizer\")\n",
    "\n",
    "    '''\n",
    "    Merge the two vectorizers using a pipeline\n",
    "    '''\n",
    "    ngrams_vectorizer = Pipeline([('feats', FeatureUnion([('word_ngram', word_vectorizer),\n",
    "                                                         ('char_ngram', char_vectorizer),\n",
    "                                                         ])),\n",
    "                                 # ('clff', LinearSVC(random_state=42))\n",
    "                                 ])\n",
    "    \n",
    "\n",
    "\n",
    "    # fitTransform this thing \n",
    "    X_train = ngrams_vectorizer.fit_transform(docs_train) #it will take a lot of time... i think\n",
    "    X_test = ngrams_vectorizer.fit_transform(docs_test)\n",
    "    print(\"Performed fitting of data\")\n",
    "    ############ perform dimensionality reduction ################\n",
    "    \n",
    "    if(perform_dimensionality_reduction == True):\n",
    "        print(\"Performing dimensionality reduction\")\n",
    "        # use TruncatedSVD to reduce dimensionality of our dataset\n",
    "        svd = TruncatedSVD(n_components = 300, random_state = 42)\n",
    "\n",
    "        X_train = svd.fit_transform(X_train)\n",
    "        X_test = svd.fit_transform(X_test)\n",
    "        print(\"Performed dimensionality reduction\")\n",
    "\n",
    "\n",
    "    # print(docs_train[0])\n",
    "    return X_train, X_test\n",
    "\n",
    "    # n-grams based on characters and words \n",
    "\n",
    "\n",
    "    # use different methods for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_model(clf, X_train, y_train, X_test, y_test):\n",
    "    # training phase\n",
    "    # fit the X_train, y_train on the clf\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    '''\n",
    "    Predict the output of the test set\n",
    "    '''\n",
    "    y_predicted = clf.predict(X_test)\n",
    "\n",
    "    '''\n",
    "    Build the confusion matrix\n",
    "    '''\n",
    "    # confusion_matrix = metrics.confusion_matrix(y_test, y_predicted)\n",
    "\n",
    "    # # plt.imshow(confusion_matrix)\n",
    "\n",
    "    # # plt.set_cmap('jet')\n",
    "\n",
    "    # # plt.show()\n",
    "\n",
    "\n",
    "    ###################### print the accuracy of our classifier ###########################\n",
    "    accuracy = metrics.accuracy_score(y_test, y_predicted)\n",
    "    print(f'Accuracy of our classifier is : {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the project...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'entry' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-d1328560737f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m### 1 -> Read the data from the files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmerged_tweets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthor_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_tweet_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_pan_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'xmls_directory'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'truth_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'txts_destination_directory'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loaded Pan data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-65-5c4a7e68acb0>\u001b[0m in \u001b[0;36mload_pan_data\u001b[0;34m(xmls_directory, truth_path, txts_destination_directory)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mtruths_temp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mtruths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'entry' is not defined"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted the records based on author ids\n"
     ]
    }
   ],
   "source": [
    "author_ids_train, docs_train, y_train, original_tweet_lengths_train = [list(tuple) for tuple in zip(*sorted(zip(\n",
    "        author_ids_train, docs_train, y_train, original_tweet_lengths_train)))]\n",
    "    # Sort the test set\n",
    "author_ids_test, docs_test, y_test, original_tweet_lengths_test = [list(tuple) for tuple in zip(*sorted(zip(\n",
    "    author_ids_test, docs_test, y_test, original_tweet_lengths_test)))]\n",
    "\n",
    "print(\"Sorted the records based on author ids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# decision tree classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#random forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#naive bayes classifier\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built a classifier\n"
     ]
    }
   ],
   "source": [
    "#########################################################################\n",
    "'''\n",
    "Try different classifiers and compare their accuracies\n",
    "'''\n",
    "# make a classifier\n",
    "clf = LinearSVC(random_state = 42) # --> 0.5508\n",
    "\n",
    "################ different classifiers ##########################\n",
    "# Naive Bayes classifier\n",
    "# clf = GaussianNB() ## --> 0.5275\n",
    "\n",
    "\n",
    "# Random Forest classifier\n",
    "# clf = RandomForestClassifier(n_estimators=250) #--> 0.521\n",
    "\n",
    "\n",
    "# Decision Tree classifier\n",
    "# clf = DecisionTreeClassifier() --> 0.51\n",
    "\n",
    "# k nearest neighbours classifier\n",
    "# clf = KNeighborsClassifier(n_neighbors=5,  metric='minkowski', p=2) #--> 0.58\n",
    "\n",
    "#########################################################################\n",
    "print(\"Built a classifier\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a word vectorizer\n",
      "Created a character vectorizer\n",
      "Performed fitting of data\n",
      "Performing dimensionality reduction\n",
      "Performed dimensionality reduction\n",
      "Successfully extracted features from the documents\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test = extract_features(docs_train, docs_test, perform_dimensionality_reduction=True)\n",
    "print(\"Successfully extracted features from the documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of our classifier is : 0.4241666666666667\n",
      "Done training of the dataset\n"
     ]
    }
   ],
   "source": [
    "    # training and testing phase\n",
    "train_and_test_model(clf, X_train, y_train, X_test, y_test)\n",
    "print(\"Done training of the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bit59e9d07f32f74406ae3baf0b290a32c6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
